Data serialization: from pickle to databases and HDF5
=====================================================

Executive summary
-----------------

When scientists have finished their data reduction tasks they need a way
to consolidate the results in persistent storage media so that they can
easily recover data afterwards.

I'll talk about the basic tools that comes with Python library for
allowing this task, as well as introducing relational databases and
general numerical oriented formats (HDF5 and NetCDF4).

The talk will be given in a tutorial style, so that people can directly
look at how things are done.  During the tutorial emphasis will be put
in comparing serialized sizes and performance.


1.- Serialization with Python basics
------------------------------------

In this section, the pickle module, and its cousin, the cPikle module,
will be introduced.  Extensive exercises will be done on how to create
persistent files out of pickled strings.

I will also introduce some interesting alternatives to pickle, most
specially wirebin [1]_, a new serializer that can perform up to 10x
faster than cPickle.

Finally, a few words about shelve, the persistent dictionary, will be
said.  I like it because of its simplicity and easy of use.  However,
attention will be given to performance issues in shelve.

.. [1] http://github.com/slideinc/wirebin


2.- Python databases
--------------------

A brief introduction to relational databases will be given: what they
are and when they should used.  Some demonstration will be done on how
to create, access and query relational databases.

Then, I'll talk on some numerical oriented formats, and in particular
HDF5 and NetCDF4, which are the standard de-facto in many fields of the
science and technology.  I'll use PyTables [2]_ for that.

Finally, some thoughts about concurrency issues for every approach will
be exposed.  Also, some hints about the advantages and drawbacks of
relational database versus a numerical oriented format will be given.

.. [2] http://www.pytables.org


3.- Adding compression into the equation
----------------------------------------

Sometimes, scientific users have a need to save really large datasets
that may eventually not fit into memory, but need to work with them in
some way or in another.

To address this, two important compressors that come with the Python
library can be handy: zlib and bz2.  I'll explain why they can be useful
for persistence and how to use them.

Finally, I'd like to do a small introduction to the Blosc compressor
[3]_ and how to use it with Pytables in order to get top-level
performance when doing I/O of extremely large datasets.

.. [3] http://blosc.pytables.org



.. Local Variables:
.. mode: rst
.. coding: utf-8
.. fill-column: 72
.. End:
