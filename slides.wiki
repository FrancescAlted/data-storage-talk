== The Basics ==

=== Introduction ===

==== Outline ====

\tableofcontents

==== Outline ====

\tableofcontents[currentsection,currentsubsection]

==== What does ''serialization'' mean? ====

\begin{quotation}
Serialization is the process of converting a data structure or object into a
sequence of bits so that it can be stored in a file or memory buffer, or
transmitted across a network connection link to be resurrected later in the
same or another computer environment.
\end{quotation}

\begin{quotation}
The basic mechanisms are to flatten object(s) into a one-dimensional
stream of bits, and to turn that stream of bits back into the original
object(s).
\end{quotation}

--1cm--

From: \href{http://www.parashift.com/c++-faq-lite/serialization.html}{http://www.parashift.com/c++-faq-lite/serialization.html}

==== Serialization tools ====

There are literally zillions of serialization tools and formats (text,
''XML'', or binary based), but well be focusing on those that are:

* Easy to use
* Space-efficient
* Fast

In particular, we are not going to discuss text-based formats (e.g.
''XML'', ''CSV'', ''YAML'', ''JSON'' ...).

==== Serialization tools that comes with Python ====

Python comes with a complete tool set of modules for serialization
purposes:

* ''pickle'', and its cousin, ''cPickle'', for quick-and-dirty serialization
* ''shelve'', a persistent dictionary based on DBM databases
* A common database API for communicating with relational databases

==== Serialization tools for binary data ====

Additionally, there are lots of third-party libraries for specialized
uses. Here will center on numerical formats:

* ''NPY'', ''NPZ'': NumPy's own format
* Wrappers for ''HDF5'', a standard de-facto format and library: ''PyTables'', ''h5py''

=== Pickling our objects ===

==== Outline ====

\tableofcontents[currentsection,currentsubsection]

==== The @pickle@ module ====

* Serializes an object into a stream of bytes
* can be saved to a file (or a string) and later restored

\pyfile{code/ex_pickle.py}

==== What does @pickle@ do ====

* It can serialize both basic Python data structures or user-defined classes.

* Always serializes data, not code (it tries to import classes if found in the pickle).
<[alertblock]{Warning}
For security reasons, programs should '''_red_NEVER_''' unpickle data received
from untrusted sources.

See also: \href{http://nedbatchelder.com/blog/201302/war\_is\_peace.html}{http://nedbatchelder.com/blog/201302/war\_is\_peace.html}
[alertblock]>

==== Its cPickle cousin ====

* Implemented in C (i.e. significantly faster than @pickle@).
* But more restrictive (does not allow subclassing of the @Pickler@ and @Unpickler@ objects).
* Python 3 @pickle@ can use the C implementation transparently.

==== Picklin' a Numpy Array ====[containsverbatim]

<[pyconcode]
>>> a = np.linspace(0, 100, 1e7)
>>> %timeit pickle.dump(a, open('p1','w'))
1 loops, best of 3: 8.92 s per loop
>>> %timeit pickle.dump(a, open('p2','w'), pickle.HIGHEST_PROTOCOL)
1 loops, best of 3: 509 ms per loop
>>> ls -sh p1 p2
186M p1   77M p2
[pyconcode]>

<[block]{}
Always try to use @cPickle@ and @HIGHEST\_PROTOCOL@
[block]>

==== @pickle/cPickle@ limitations and recommendations ====

* You need to reload all the data in the pickle before you can use any part of it. That might be inconvenient for large datasets.
* Data can only be retrieved by other Python interpreters. You loose data portability with other languages.
* Not every object in Python can be serialized by @pickle@ (e.g.  extensions).

==== Recommendations for using @pickle@ ====

* Use it mainly for small data structures.
* If you have a lot of variables that you want to save, use a dictionary for tying them together first.
* When using Ipython, be sure to use the very convenient @\%store@ magic (it uses @pickle@ under the hood).

=== The @shelve@ module ===

==== Outline ====

\tableofcontents[currentsection,currentsubsection]

==== The @shelve@ module ====

* Provides support for persistent objects using a special ''shelf'' object.
* The ''shelf'' behaves like a disk-based dictionary or ''key-value store''.
* The values of the dictionary can be any object that can be pickled.

==== Example ====[containsverbatim]

\pyfile{code/ex_shelve.py}

<[consolecode]
$ python code/ex_shelve.py
'one' 1
'three' 3
'two' 2
[consolecode]>

==== Pros and cons of the @shelve@ module ====

<[block]{_green_Pros_}
* Easy to retrieve just a selected set of variables.
* Specially handy for large pickles.
[block]>

<[block]{_red_Cons_}
* Suffers the same problems as @pickle@.
[block]>

=== Relational databases ===

==== Outline ====

\tableofcontents[currentsection, currentsubsection]

==== What's a relational database? ====

* A set of tables containing data fitted into predefined categories.
* Each table (a relation) contains one or more data categories in columns.
* Each row contains a unique instance of data for the categories defined by the columns.
* Data can be accessed in many different ways without having to reorganize the tables.

==== Terminology ====

<[center]
    <<<images/Relational_database_terms, scale=0.30>>>
[center]>


==== Base and derived relations ====

* In a relational database, all data are stored and accessed via relations.
* Relations that store data are called ''base relations'', and in implementations are called ''tables''
* Other relations do not store data, but are computed by applying relational operations to other relations.
* These relations are sometimes called ''derived relations''
* In implementations these are called ''views'' or ''queries''


==== Example of relational database ====

<[center]
    <<<images/example-reldatabase, scale=0.5>>>
[center]>


==== Queries with SQL language ====[containsverbatim]

<[block]{Simple query involving one single table (relation):}
<[minted]{mysql}
SELECT AuthorName FROM AUTHORS WHERE AuthorBDay > 1970
[minted]>
[block]>

<[block]{Complex query involving multiple relations:}
<[minted]{mysql}
SELECT AuthorName FROM AUTHORS a, BOOKS b, PUBLISHERS p
    WHERE AuthorBDay > 1970
        AND a.AuthorID = b.AuthorID
        AND b.PubID = p.PubID
        AND p.Publisher = "Random House"
    GROUP BY AuthorBDay
[minted]>
[block]>

<[alertblock]{Beware}
complex queries can consume a lot of resources!
[alertblock]>

==== Relational database API specification ====

* The Python community has developed a standard API for accessing relational databases in a uniform way (PEP 249).
* Specific database modules (e.g. MySQL, Oracle, Postgres ...) follow this specification, but may add more features.
* Python comes with SQLite.
** SQL Database, but stored as a single file on disk.
** A relational database accessible via the @sqlite3@ module.

==== ORM (Object Relational Mapping) ====

* The relational database API in Python is powerful, but pretty rough to use and '''not''' object-oriented.
* Many projects have appeared to add an object-oriented layer on top of this API:
** SQLAlchemy
** Django's native ORM
** Storm
** Elixir
** SQLObject (the one that started it all)
** ... probably a lot more ...

==== Define Objects ====

\pyfile[firstline=1, lastline=15]{code/ex_storm.py}

==== Setup database ====

\pyfile[firstline=17, lastline=23]{code/ex_storm.py}

==== Add Flowers ====

\pyfile[firstline=25, lastline=37]{code/ex_storm.py}

==== Add Vases and commit ====

\pyfile[firstline=39, lastline=48]{code/ex_storm.py}

==== Search and Retrieve ====

\pyfile[firstline=50, lastline=60]{code/ex_storm.py}

==== Executing ====[containsverbatim]

<[consolecode]
$ python code/ex_storm.py
[(u'Flowers', u'Red Rose'), (u'Flowers', u'Violet')]
[(u'Vases', u'Amphora')]
[consolecode]>

==== RDBMs highlights ====

They offer ''ACID'' (atomicity, consistency, isolation, durability) properties, that can be translated into:

* Referential integrity.
* Transaction support.
* Data consistency.

+ Indexing capabilities (accelerate queries in large tables).

--1cm--

But this comes with a price...

==== RDBMs drawbacks ====

* Insertions are SLOOOW.
* Not very space-efficient.
* Not well adapted to handle large numerical datasets (no direct interface with NumPy).
* You need a knowledgeable RDBM administrator to squeeze all the performance out of them.


== Numerical Binary Formats ==

=== Why we need them? ===

==== Outline ====

\tableofcontents[currentsection,currentsubsection]

==== What's a numerical binary format? ====

* It is a format specialized in saving and retrieving large amounts of numerical data.
* Usually come with libraries that can understand that format.
* They range from the very simple (NPY) to rather complex and powerful (HDF5).
* There are a really huge number of numerical formats depending on the needs. Will focus on just on few.

==== Why we need a binary format? ====

* They are closer to memory representation.
* Their representation is space-efficient (1 byte in-memory \ensuremath{\approx} 1 bytes on disk).
* They are CPU-friendly (in general you do not have to convert from one representation to another).

==== NumPy: the real cornerstone of numerical interfaces ====

* NumPy is the standard de-facto for dealing with numerical data in-memory.
* Hence, most of the interfaces to numerical formats in the Python world use NumPy to interact with the database.
* In some cases the integration is so tight that it could be difficult to say if you are working with NumPy or the interface.

=== The NPY format ===

==== The NPY format ====

* Created back in 2007 for overcoming limitations of @pickle@ for NumPy arrays as well as @numpy.tofile()@ / @numpy.fromfile()@ functions
* It is a binary format, so it is space-efficient.
* It comes integrated with NumPy.

* See also: \href{https://github.com/numpy/numpy/blob/master/doc/neps/npy-format.txt}{A Simple File Format for NumPy Arrays}

==== NPY exposes the simplest API for NumPy ====[containsverbatim]

\pyfile{code/ex_npy.py}

==== What is in the file? ====[containsverbatim]

<[consolecode]
<[nowiki]
$ head -c 100 test.npy 
NUMPYF{'descr': '<f8', 'fortran_order': False, 'shape': (10000000,), }     
ð?% 
$ head -c 100 test.npy | xxd
0000000: 934e 554d 5059 0100 4600 7b27 6465 7363  .NUMPY..F.{'desc
0000010: 7227 3a20 273c 6638 272c 2027 666f 7274  r': '<f8', 'fort
0000020: 7261 6e5f 6f72 6465 7227 3a20 4661 6c73  ran_order': Fals
0000030: 652c 2027 7368 6170 6527 3a20 2831 3030  e, 'shape': (100
0000040: 3030 3030 302c 292c 207d 2020 2020 200a  00000,), }     .
0000050: 0000 0000 0000 0000 0000 0000 0000 f03f  ...............?
0000060: 0000 0000                                ....
[nowiki]>
[consolecode]>

... just a header plus binary ...


==== Memory-mapping and NPY ====[containsverbatim]

You can open a NPY file in memmap-mode for accessing data directly
from disk:

<[pyconcode]
>>> mmdata = np.load('test.npy', mmap_mode='r+')
>>> mmdata
memmap([  0.00000000e+00,   1.00000000e+00,   2.00000000e+00, ...,
         9.99999700e+06,   9.99999800e+06,   9.99999900e+06])
>>> mmdata[-10:] + mmdata[:10]
memmap([  9999990.,   9999992.,   9999994.,   9999996.,   9999998.,
        10000000.,  10000002.,  10000004.,  10000006.,  10000008.])
>>> del mmdata # close access to 'test.npy'
[pyconcode]>


==== Saving several arrays with NPZ ====[containsverbatim]

The NPY format has a special mode that can save several arrays in
one single ZIP file (but no compression is used at all!):

\pyfile{code/ex_npz.py}

Just a ZIP file.

<[consolecode]
$ python ex_npz.py
$ file test.npz
test.npz: Zip archive data, at least v2.0 to extract
[consolecode]>

==== Loading several arrays with NPZ ====[containsverbatim]

<[pyconcode]
>>> arrs = np.load('test.npz')
>>> arrs.items()
[('a',
  array([  0.00000000e+00,   1.00000010e-05,   2.00000020e-05, ...,
         9.99999800e+01,   9.99999900e+01,   1.00000000e+02])),
 ('sina',
  array([  0.00000000e+00,   1.00000010e-05,   2.00000020e-05, ...,
        -5.06382887e-01,  -5.06374264e-01,  -5.06365641e-01]))]

>>> pylab.plot(arrs['a'], arrs['sina'])
[pyconcode]>

<[center]
    <<<images/npz.pdf, scale=0.30>>>
[center]>

==== Pros and cons of NPY ====

<[block]{Pros:}
* Binary format, so space-efficient.
* Avoids duplication of data in memory during saving/loading operations.
* Array data accessible through memory-mapping.
[block]>

<[block]{Cons:}
* The memory mapping feature only allows to deal with files that do not exceed the available virtual memory.
* Non-standard format outside the NumPy community.
* No other features than basic input/output (e.g. no metadata allowed).
* No compression
[block]>



=== The HDF5 format ===

==== Outline ====

\tableofcontents[currentsection,currentsubsection]

==== The HDF5 format ====

* HDF5 (Hierarchical Data Format v5) is a library and file format for storing and managing any kind of data.
* \href{http://www.hdfgroup.org/HDF5/doc/H5.format.html}{http://www.hdfgroup.org/HDF5/doc/H5.format.html}
* It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data.
* Originally developed at the NCSA, and currently maintained by The THG Group, a not for-profit organization.
* HDF5 has been around for over twenty years, and has become a standard de-facto format supported by many applications (MatLab, IDL, R, Mathematica ...).

==== Outstanding features of HDF5 ====

* Can store all kinds of data in a variety of ways.
* Runs on most systems.
* Lots of tools to access data.
* Long term format support (HDF-EOS, CGNS).
* Library and format emphasis on I/O efficiency and different kinds of storage.

==== Python interfaces ====

* '''h5py''' is an attempt to map the HDF5 feature set to Python as closely as possible.
** It also provides access to nearly all of the HDF5 C API (the so-called low-level API).
** Not designed to go beyond HDF5/NumPy capabilities.

* '''PyTables''' builds up an additional abstraction layer on top of HDF5 and NumPy where it implements things like:
** An enhanced type system (enumerated, time, variable length types and default values supported).
** An engine for enabling complex queries and out-of-core computations (using Numexpr behind the scenes).
** Advanced indexing capabilities (Optimally Partially Sorted Indices, OPSI)

==== Creating an HDF5 file ====[containsverbatim]

\pyfile[firstline=1, lastline=6]{code/ex_tables.py}

<[pyconcode]
>>> ds
/reduced_data/array (Array(4,)) ''
  atom := Int64Atom(shape=(), dflt=0)
  maindim := 0
  flavor := 'numpy'
  byteorder := 'little'
  chunkshape := None
[pyconcode]>

==== Creating an Table ====[containsverbatim]

\pyfile[firstline=8, lastline=10]{code/ex_tables.py}

<[pyconcode]
>>> tab
/table (Table(1000000,)) ''
  description := {
  "f0": Int32Col(shape=(), dflt=0, pos=0),
  "f1": Int64Col(shape=(), dflt=0, pos=1),
  "f2": Float64Col(shape=(), dflt=0.0, pos=2)}
  byteorder := 'little'
  chunkshape := (6553,)
[pyconcode]>

==== Querying a Table ====[containsverbatim]

<[pyconcode]
>>> tab[3]
(3, 6, 27.0)
>>> tab[3:2000]
array([(3, 6, 27.0), (4, 8, 64.0), (5, 10, 125.0), ...,
       (1997, 3994, 7964053973.0), (1998, 3996, 7976023992.0),
       (1999, 3998, 7988005999.0)], 
      dtype=[('f0', '<i4'), ('f1', '<i8'), ('f2', '<f8')])

>>> tab[[3,100]]
array([(3, 6, 27.0), (100, 200, 1000000.0)], 
      dtype=[('f0', '<i4'), ('f1', '<i8'), ('f2', '<f8')])

>>> [v[:] for v in tab.where("(f0 > 1) & (f2 < 100)")]
[(2, 4, 8.0), (3, 6, 27.0), (4, 8, 64.0)]
[pyconcode]>

==== Modifying a Table ====[containsverbatim]

<[pyconcode]
>>> tab[0] = (3, 3, 3.0)
>>> tab[:4]
array([(3, 3, 3.0), (1, 2, 1.0), (2, 4, 8.0), (3, 6, 27.0)], 
      dtype=[('f0', '<i4'), ('f1', '<i8'), ('f2', '<f8')])

>>> tab[[1, 3]] = [(4, 4, 4.0)]*2
>>> tab[:4]
array([(3, 3, 3.0), (4, 4, 4.0), (2, 4, 8.0), (4, 4, 4.0)], 
      dtype=[('f0', '<i4'), ('f1', '<i8'), ('f2', '<f8')])

>>> for row in tab.where("(f0 < 4) & (f2 <= 8.)"):
....:     row['f1'] = 0
....:     row.update()
....: 
>>> tab[:4]
array([(3, 0, 3.0), (4, 4, 4.0), (2, 0, 8.0), (4, 4, 4.0)], 
      dtype=[('f0', '<i4'), ('f1', '<i8'), ('f2', '<f8')])
[pyconcode]>

==== Annotating you Datasets ====[containsverbatim]

<[pyconcode]
>>> print tab
/table (Table(1000000,)) ''
>>> tab.attrs.TITLE = "sample data"
>>> print tab
/table (Table(1000000,)) 'sample data'
>>> tab.attrs.CLASS
'TABLE'
>>> tab.attrs.mycomment = "Enjoy data!"
>>> tab.attrs.complementary_data = np.array([3,2,3])
>>> tab.attrs.complementary_data
array([3, 2, 3])
[pyconcode]>
%''

== Adding Compression ==

=== Why compression? ===

==== Outline ====

\tableofcontents[currentsection,currentsubsection]

==== Why compression? ====

* Files takes less space (the obvious reason).
* I/O speed can benefit a lot.
* If compression speed is good enough, it is a nice way to shelve arrays in-memory.

==== Two compression paradigms ====

<[block]{Solid}
* Data is compressed and decompressed as a whole.
* A compressed buffer must be decompressed completely before usage.
* The typical case is compressing a pickle.
[block]>

<[block]{Chunked}
* Data is stored compressed in chunks and a chunk is decompressed only when it is needed.
* Typical case is HDF5 / NetCDF4 files (or compressed filesystems).
[block]>

=== Solid Compression ===

==== Outline ====

\tableofcontents[currentsection,currentsubsection]

==== Solid compression ====[containsverbatim]

* Python comes with a series of codecs (compressor/decompressor) that are ready to use.
* One approach is to compress a pickle:

<[pyconcode]
>>> a = np.linspace(0, 100, 1e7)
>>> pa = cPickle.dumps(a, cPickle.HIGHEST_PROTOCOL)
>>> a.size*a.itemsize, len(pa)
(80000000, 80000135)

>>> zpa = zlib.compress(pa, 9)
>>> len(pa), len(zpa), float(len(zpa)) / len(pa)
(80000135, 52946378, 0.6618286081642237)

>>> bpa = blosc.compress(pa, a.itemsize, 9)
>>> len(pa), len(bloscpa), float(len(bpa)) / len(pa)
(80000135, 7634771, 0.09543447645432099)
[pyconcode]>

==== I/O with a compressed pickle ====[containsverbatim]

* Compressed pickles can be saved easily.
* Simply treat them as binary streams.

--1cm--

<[pyconcode]
>>> with open("my_cpickle.bin", "wb") as f:
...:    f.write(zpa)
>>> with open("my_cpickle.bin", "rb") as f:
...:    zpar = f.read()
>>> assert zpa == zpar
[pyconcode]>

==== Unpickling a compressed pickle ====[containsverbatim]

* Just decompress it first

--1cm--

<[pyconcode]
>>> pad = zlib.decompress(zpar)
>>> assert pa == pad
>>> a2 = cPickle.loads(pad)
>>> np.alltrue(a == a2)
True
[pyconcode]>


==== Sneak preview: Bloscpack ====[containsverbatim]

* An alternative is to use the @\href{https://github.com/esc/bloscpack}{bloscpack}@ command line comprssion tool on the NPY/NPZ data

<[consolecode]
$ ls -sh test.npy
77M test.npy
$ bloscpack compress --level 9 test.npy
$ ls -sh test.npy.blp
668K test.npy.blp
[consolecode]>

* Compression ratio: 0.008470

<[consolecode]
$ ls -sh test.npz
153M test.npz
$ bloscpack compress --level 9 test.npz
$ ls -sh test.npz.blp
52M test.npz.blp
[consolecode]>

* Compression ratio: 0.337617

==== Resource consumption for solid compression ====

<[block]{Memory:}
You need to book some spare memory to keep the compressed pickle.
[block]>

<[block]{CPU:}
Compressors consume quite a lot of it, but you may always find a compressor that fits your needs.
[block]>

For example, for a pickle of @np.linspace(0, 100, 1e7)@:

<[nowiki]
\begin{center}
\texttt{\footnotesize }\begin{tabular}{|c|c|c|c|c|}
\hline 
(all compressors with level 9) & memcpy & blosc & zlib & bzip2\tabularnewline
\hline
\hline 
final size (MB) & 76 & 7.7 & 50 & 55\tabularnewline
\hline 
compress throughput (MB/s) & 3500 & 3600 & 4.8 & 4.5\tabularnewline
\hline 
decompress throughput (MB/s) & 3500 & 3500 & 120 & 9.9\tabularnewline
\hline
\end{tabular}
\par\end{center}{\footnotesize \par}

\textrm{\textit{Hardware: 2 x Intel E5520 @ 2.27GHz, 8 MB third level
cache.}}
[nowiki]>

=== Chunked compression ===

==== Outline ====

\tableofcontents[currentsection,currentsubsection]

==== Chunked compression ====

* Data is stored compressed in chunks (on-disk or in-memory)
* Chunks are decompressed when needed only.
* HDF5 / NetCDF4 support this paradigm.
* Saves disk and memory resources and may, in some situations, even accelerate the I/O speed.

==== Examples with PyTables/HDF5 ====

* PyTables includes support for a fair number of compressors: Zlib, Bzip2, LZO and Blosc.
* It also supports ''shuffle'', an interesting filter designed to improved compression ratios.
* You can choose whatever combination that proves to be more convenient for your needs.


==== Querying compressed data ====

The dataset is a table with real data used in astronomy:
* 30 columns, most of them floating points and some ints
* Around 77000 entries
* Query: all entries where @ra@ > 19 (3\% selectivity)

\begin{center}
\texttt{\footnotesize }\begin{tabular}{|c|c|c|c|c|}
\hline 
(all compressors with level 5) & no compr & blosc & zlib & bzip2\tabularnewline
\hline
\hline 
table size (MB) & 10 & 5.3 & 4.7 & 4.6\tabularnewline
\hline 
creation throughput (MB/s) & 330 & 250 & 22 & 7.7\tabularnewline
\hline 
query throughput (MB/s) & 170 & 140 & 38 & 10\tabularnewline
\hline
\end{tabular}
\par\end{center}{\footnotesize \par}

==== Effect of chunked compression when writing large datasets ====

<[center]
    <<<images/create-chunksize-30GB, scale=0.40>>>
[center]>

==== Effect of chunked compression when reading large datasets ====

<[center]
    <<<images/sequential-8MB-30GB, scale=0.40>>>
[center]>


==== When should you use compression? ====

* Your data has to be compressible (sparse matrices, time series, data with low entropy, ...).
* Whether your disk space is tight or your datasets are large.
* You want to optimize I/O speed.

== Summary ==

==== Outline ====

\tableofcontents[currentsection,currentsubsection]

==== Summary ====

* Pickle is the most basic, but still powerful, way to serialize Python data.
** But it is mainly meant for small datasets and it is not portable.
* Relational databases are portable, mature and solid as a rock.
** However, they do not interact well with NumPy and write performance is pretty lame.
* HDF5 shows best performance.
** Python APIs interacts well with NumPy and are extremely portable.
** They lack safety features.
* Using compression allows you to deal with more data using the same resources.
** In general, they can save I/O time to disk.
